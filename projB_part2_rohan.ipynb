{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import Dataset, NormalPredictor, Reader, SVD, accuracy, Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_set_df = pd.read_csv(\"./data_movie_lens_100k/ratings_all_development_set.csv\")\n",
    "users_df = pd.read_csv(\"./data_movie_lens_100k/user_info.csv\")\n",
    "movies_df = pd.read_csv(\"./data_movie_lens_100k/movie_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  item_id  rating\n",
      "0          772       36       3\n",
      "1          471      228       5\n",
      "2          641      401       4\n",
      "3          312       98       4\n",
      "4           58      504       5\n",
      "...        ...      ...     ...\n",
      "89987      415      813       4\n",
      "89988      842      120       3\n",
      "89989      574      505       2\n",
      "89990      757      472       5\n",
      "89991      503      204       3\n",
      "\n",
      "[89992 rows x 3 columns]\n",
      "     user_id  age  is_male  orig_user_id\n",
      "0          0   24        1             1\n",
      "1          1   53        0             2\n",
      "2          2   23        1             3\n",
      "3          3   24        1             4\n",
      "4          4   33        0             5\n",
      "..       ...  ...      ...           ...\n",
      "938      938   26        0           939\n",
      "939      939   32        1           940\n",
      "940      940   20        1           941\n",
      "941      941   48        0           942\n",
      "942      942   22        1           943\n",
      "\n",
      "[943 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ratings_set_df)\n",
    "print(users_df)\n",
    "#print(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ratings_features = ratings_set_df[[\"user_id\", \"item_id\"]]\n",
    "ratings = ratings_set_df[\"rating\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(ratings_features, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_valid_test_loader import load_train_valid_test_datasets\n",
    "\n",
    "# Load the dataset in the same way as the main problem \n",
    "train_tuple, valid_tuple, test_tuple, n_users, n_items = \\\n",
    "        load_train_valid_test_datasets()\n",
    "\n",
    "\n",
    "def tuple_to_surprise_dataset(tupl):\n",
    "    \"\"\"\n",
    "    This function convert a subset in the tuple form to a `surprise` dataset. \n",
    "    \"\"\"\n",
    "    ratings_dict = {\n",
    "        \"userID\": tupl[0],\n",
    "        \"itemID\": tupl[1],\n",
    "        \"rating\": tupl[2],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(ratings_dict)\n",
    "\n",
    "    # A reader is still needed but only the rating_scale param is requiered.\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "    # The columns must correspond to user id, item id and ratings (in that order).\n",
    "    dataset = Dataset.load_from_df(df[[\"userID\", \"itemID\", \"rating\"]], reader)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "## Below we train an SVD model and get its vectors \n",
    "\n",
    "# train an SVD model using the training set\n",
    "trainset = tuple_to_surprise_dataset(train_tuple).build_full_trainset()\n",
    "test_trainset = tuple_to_surprise_dataset(test_tuple).build_full_trainset()\n",
    "valid_trainset = tuple_to_surprise_dataset(valid_tuple).build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_features_for_user(user_id):\n",
    "    return np.array(users_df[users_df[\"user_id\"] == user_id].iloc[0][[\"age\", \"is_male\"]])\n",
    "\n",
    "def get_features_for_item(item_id):\n",
    "    return np.array(movies_df[movies_df[\"item_id\"] == item_id].iloc[0][[\"release_year\"]])\n",
    "\n",
    "def get_rating(user_id, item_id, features_df: pd.DataFrame):\n",
    "    return features_df[\n",
    "        (features_df[\"user_id\"] == user_id) &\n",
    "            (features_df[\"item_id\"] == item_id)\n",
    "    ].iloc[0][\"rating\"]\n",
    "\n",
    "def get_feature_vectors(data_tuple, svd: SVD, trainset: Trainset):\n",
    "    user_vectors = svd.pu\n",
    "    item_vectors = svd.qi\n",
    "    \n",
    "    features = np.zeros(\n",
    "        (\n",
    "            len(data_tuple[0]),\n",
    "            user_vectors.shape[1] + item_vectors.shape[1] + len([\"age\", \"is_male\"]) + len([\"release_year\"])\n",
    "        )\n",
    "    )\n",
    "    ratings = []\n",
    "    for index in range(0, len(data_tuple[0])):\n",
    "        user_id = data_tuple[0][index]\n",
    "        item_id = data_tuple[1][index]\n",
    "        u_i = np.zeros((user_vectors.shape[1]))\n",
    "        if trainset.knows_user(user_id):\n",
    "            u_i = user_vectors[trainset.to_inner_uid(user_id)]\n",
    "        m_j = np.zeros((item_vectors.shape[1]))\n",
    "        if trainset.knows_item(item_id):\n",
    "            m_j = item_vectors[trainset.to_inner_iid(item_id)]\n",
    "        user_features = get_features_for_user(user_id)\n",
    "        item_features = get_features_for_item(item_id)\n",
    "        feature_vector = np.concatenate(\n",
    "            (\n",
    "                u_i,\n",
    "                m_j,\n",
    "                user_features,\n",
    "                item_features,\n",
    "            ),\n",
    "            axis=0\n",
    "        )\n",
    "        features[index] = feature_vector\n",
    "        \n",
    "        ratings.append(1 if data_tuple[2][index] > 4.5 else 0)\n",
    "    \n",
    "    return features, ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svd = SVD(\n",
    "    n_factors = 10,\n",
    "    n_epochs=50\n",
    ")\n",
    "\n",
    "svd.fit(trainset)\n",
    "\n",
    "\n",
    "features_train, ratings_train = get_feature_vectors(train_tuple, svd, trainset)\n",
    "features_test, ratings_test = get_feature_vectors(test_tuple, svd, test_trainset)\n",
    "features_val, ratings_val = get_feature_vectors(valid_tuple, svd, valid_trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# mlp = MLPClassifier(\n",
    "# )\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# features_train_scaled = scaler.fit_transform(features_train)\n",
    "\n",
    "# param_grid = {\n",
    "#     \"solver\": [\"adam\", \"sgd\"],\n",
    "#     \"activation\": [\"relu\", \"tanh\"],\n",
    "#     \"hidden_layer_sizes\": [(100,), (50,)],\n",
    "# }\n",
    "\n",
    "# gs = GridSearchCV(\n",
    "#     estimator=mlp,\n",
    "#     param_grid=param_grid,\n",
    "#     refit=True,\n",
    "#     return_train_score=True, \n",
    "    \n",
    "# )\n",
    "\n",
    "# gs.fit(features_train, ratings_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    " \n",
    "# roc_disp = RocCurveDisplay.from_estimator(gs.best_estimator_, features_train, ratings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# svm = SVC(probability=True)\n",
    "\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'gamma': [0.01, 0.1, 1],\n",
    "#     'kernel': ['rbf']\n",
    "# }\n",
    "\n",
    "# gs = GridSearchCV(\n",
    "#     estimator=svm,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=3,\n",
    "#     scoring='roc_auc',\n",
    "#     refit=True\n",
    "# )\n",
    "\n",
    "# gs.fit(features_train, ratings_train)\n",
    "\n",
    "# print(gs.best_estimator_)\n",
    "# print(gs.best_score_)\n",
    "\n",
    "# # best_svm = SVC(**best_params, probability=True)\n",
    "# # best_svm.fit(features_train, ratings_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 30\u001b[0m\n\u001b[1;32m     21\u001b[0m forest_searcher \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mbase_forest,\n\u001b[1;32m     22\u001b[0m                                 param_grid\u001b[38;5;241m=\u001b[39mforest_hyperparameter_grid_by_name,\n\u001b[1;32m     23\u001b[0m                                 scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m                                 cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     25\u001b[0m                                 return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m                                 refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[1;32m     28\u001b[0m forest_searcher\u001b[38;5;241m.\u001b[39mfit(features_train, ratings_train)\n\u001b[0;32m---> 30\u001b[0m best_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mforest_searcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m \n\u001b[1;32m     31\u001b[0m best_Score \u001b[38;5;241m=\u001b[39m forest_searcher\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "base_forest = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini',\n",
    "    max_depth=16,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1)\n",
    "\n",
    "forest_hyperparameter_grid_by_name = dict(\n",
    "    max_features=[3, 10, 33, 100, 333],\n",
    "    max_depth=[16, 32, 64],\n",
    "    min_samples_leaf=[1],\n",
    "    n_estimators=[50, 100, 200],\n",
    "    random_state=[101],\n",
    "    )\n",
    "\n",
    "forest_searcher = GridSearchCV(estimator=base_forest,\n",
    "                                param_grid=forest_hyperparameter_grid_by_name,\n",
    "                                scoring='roc_auc',\n",
    "                                cv=3,\n",
    "                                return_train_score=True,\n",
    "                                refit=False) \n",
    "\n",
    "forest_searcher.fit(features_train, ratings_train)\n",
    "\n",
    "best_estimator = forest_searcher.best_estimator_ \n",
    "best_Score = forest_searcher.best_score_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 64, 'max_features': 3, 'min_samples_leaf': 1, 'n_estimators': 200, 'random_state': 101}\n",
      "0.8083570788158362\n"
     ]
    }
   ],
   "source": [
    "print(forest_searcher.best_params_)\n",
    "print(forest_searcher.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "new_forest = RandomForestClassifier(\n",
    "    max_depth=64,\n",
    "    max_features=3,\n",
    "    min_samples_leaf=1,\n",
    "    n_estimators = 200,\n",
    "    random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_test_data = pd.read_csv(\"data_movie_lens_100k/ratings_masked_leaderboard_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Applications/anaconda3/envs/cs135_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m svd \u001b[38;5;241m=\u001b[39m SVD()\n\u001b[1;32m      3\u001b[0m svd\u001b[38;5;241m.\u001b[39mfit(trainset)\n\u001b[0;32m----> 5\u001b[0m features_test, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_feature_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_test_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 18\u001b[0m, in \u001b[0;36mget_feature_vectors\u001b[0;34m(data_tuple, svd, trainset)\u001b[0m\n\u001b[1;32m     13\u001b[0m user_vectors \u001b[38;5;241m=\u001b[39m svd\u001b[38;5;241m.\u001b[39mpu\n\u001b[1;32m     14\u001b[0m item_vectors \u001b[38;5;241m=\u001b[39m svd\u001b[38;5;241m.\u001b[39mqi\n\u001b[1;32m     16\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m     17\u001b[0m     (\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m),\n\u001b[1;32m     19\u001b[0m         user_vectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m item_vectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_male\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelease_year\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data_tuple[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m     23\u001b[0m     user_id \u001b[38;5;241m=\u001b[39m data_tuple[\u001b[38;5;241m0\u001b[39m][index]\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/cs135_env/lib/python3.10/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/cs135_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "user_ids = masked_test_data[\"user_id\"].values\n",
    "item_ids = masked_test_data[\"item_id\"].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs135_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
